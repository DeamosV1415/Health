{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the necessary imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_tavily import TavilySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d465fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing State\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f83f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralSearch(BaseModel):\n",
    "    \"\"\"\"Input for the general search tool\"\"\"\n",
    "    query: str = Field(default=\"\", description=\"Search query\")\n",
    "\n",
    "# class Chatbot_Output(BaseModel):\n",
    "#   \"\"\"Output Schema for the Chatbot\"\"\"\n",
    "#   reply: str = Field(default=\"\", description=\"Chatbot reply\")\n",
    "#   need_clarification: bool = Field(default=False, description=\"Need clarification. Only true when the chatbot needs clarification.\")\n",
    "#   search_tasks: list = Field(default=[], description=\"Search tasks for the tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab920235",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=GeneralSearch)\n",
    "def general_search(query: str):\n",
    "    \"\"\"General search tool\"\"\"\n",
    "    general_search = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\"\n",
    ")\n",
    "    return general_search.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a medical information assistant. \n",
    "When a user asks a health question:\n",
    "1. Use the general_search tool to find accurate medical information\n",
    "2. After receiving search results, provide a clear, helpful answer\n",
    "3. Include appropriate medical disclaimers when needed.\n",
    "4. Always be sure to explain as if the user is in 5th grade. If some complex medical terms come up, be sure to simplify them. Don't use too much medical jargon.\n",
    "Important: \n",
    "- Respond naturally in plain text, not JSON\n",
    "- After you get tool results, synthesize them into a helpful answer\n",
    "- Always inclue some relevant medical questions for your doctor about the query.\n",
    "- For every query, when you give you verdict, always give a coloured alert. \n",
    "- Give a coloured alert to the user. üü¢ Green if the user is safe, üü° Yellow if the user is at risk, üü† Orange if the user is at high risk, üî¥ Red if the user is at very high risk.\n",
    "- Following the colour alert, add an advise to the user.\n",
    "After this continue with all the other important steps.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addfc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM with tools\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([general_search])\n",
    "\n",
    "def chatbot_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process messages and decide if tools are needed\"\"\"\n",
    "\n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "\n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }\n",
    "    \n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if we need to call tools\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"chatbot\", chatbot_node)\n",
    "workflow.add_node(\"tools\", ToolNode(tools=[general_search]))\n",
    "\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f063a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae241ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        # Extract user message content\n",
    "        user_message = message if isinstance(message, str) else message.get(\"text\", \"\")\n",
    "        \n",
    "        # Invoke the graph\n",
    "        result = graph.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_message)]}, \n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Return the assistant's response in proper format\n",
    "        assistant_response = result[\"messages\"][-1].content\n",
    "        return assistant_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Chat error: {e}\")\n",
    "        return (f\"Sorry, I encountered an error. -> {e}\")\n",
    "\n",
    "gr.ChatInterface(\n",
    "    chat,\n",
    "    title=\"Health Chatbot\",\n",
    "    description=\"Hi! What can I help you with?\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Health Chatbot Gradio Interface\n",
    "Provides a web interface with voice input support for the health chatbot.\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "from agent import get_chatbot_response\n",
    "\n",
    "# Optional: Uncomment to enable audio transcription with OpenAI Whisper\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Transcribe audio file to text using Whisper API.\n",
    "    \n",
    "    To enable this:\n",
    "    1. pip install openai\n",
    "    2. Uncomment the OpenAI import at the top\n",
    "    3. Ensure OPENAI_API_KEY is in your .env file\n",
    "    \"\"\"\n",
    "    # Uncomment the following to enable transcription:\n",
    "    try:\n",
    "        with open(audio_path, \"rb\") as audio_file:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file\n",
    "            )\n",
    "        return transcript.text\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Placeholder - transcription not enabled\n",
    "    return None\n",
    "\n",
    "\n",
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    Chat function that handles both text and audio inputs.\n",
    "    \n",
    "    Args:\n",
    "        message: Either a string (text) or dict with 'text' and 'files' keys (multimodal)\n",
    "        history: Chat history (managed by Gradio)\n",
    "        \n",
    "    Returns:\n",
    "        The chatbot's response string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_message = \"\"\n",
    "        \n",
    "        # Handle multimodal input (text + audio)\n",
    "        if isinstance(message, dict):\n",
    "            user_message = message.get(\"text\", \"\")\n",
    "            audio_files = message.get(\"files\", [])\n",
    "            \n",
    "            # If audio is provided, try to transcribe it\n",
    "            if audio_files:\n",
    "                audio_path = audio_files[0]  # Get first audio file\n",
    "                transcribed_text = transcribe_audio(audio_path)\n",
    "                \n",
    "                if transcribed_text:\n",
    "                    # Use transcribed text if no text was provided\n",
    "                    user_message = user_message or transcribed_text\n",
    "                elif not user_message:\n",
    "                    # No transcription available and no text provided\n",
    "                    return (\"üé§ **Audio transcription is not yet enabled.**\\n\\n\"\n",
    "                           \"To enable voice input:\\n\"\n",
    "                           \"1. Install: `pip install openai`\\n\"\n",
    "                           \"2. Add your `OPENAI_API_KEY` to .env\\n\"\n",
    "                           \"3. Uncomment the transcription code in gradio_app.py\\n\\n\"\n",
    "                           \"For now, please type your question.\")\n",
    "        else:\n",
    "            # Plain text input\n",
    "            user_message = message\n",
    "        \n",
    "        # Validate we have a message\n",
    "        if not user_message or not user_message.strip():\n",
    "            return \"Please provide a message or voice input.\"\n",
    "        \n",
    "        # Get response from the agent\n",
    "        response = get_chatbot_response(user_message)\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Chat error: {e}\")\n",
    "        return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"üè• Health Chatbot\",\n",
    "    description=(\n",
    "        \"Hi! I'm your health information assistant. Ask me about symptoms, \"\n",
    "        \"conditions, or general health questions.\\n\\n\"\n",
    "        \"üí¨ You can type your question or üé§ use voice input (if enabled).\"\n",
    "    ),\n",
    "    multimodal=True,  # Enables audio input button\n",
    "    examples=[\n",
    "        \"What are the symptoms of flu?\",\n",
    "        \"How can I improve my sleep quality?\",\n",
    "        \"What should I do if I have a headache?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch the interface\n",
    "    demo.launch(\n",
    "        share=False,  # Set to True to create a public link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=7860\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
